{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54402719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (22.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (65.6.3)\n",
      "Requirement already satisfied: wheel in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.38.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U pip setuptools wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e6cf6",
   "metadata": {},
   "source": [
    "Spacy for Natural Language Processing.\n",
    "\n",
    "STOP_WORDS is a set of default stop words for English language model in SpaCy.\n",
    "\n",
    "punctuation is a pre-initialized string which will give the all sets of punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "056026d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content. Artificial intelligence algorithms are commonly developed and employed to achieve this, specialized for different types of data.\n",
    "\n",
    "Text summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document[1]. On the other hand, visual content can be summarized using computer vision algorithms. Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection[2][3][4]. Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion[5][6][7][8]. Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.\n",
    "Extraction-based summarization\n",
    "Here, content is extracted from the original data, but the extracted content is not modified in any way. Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).\n",
    "Abstractive-based summarization\n",
    "Abstractive summarization methods generate new text that did not exist in the original text.[12] This has been applied mainly for text. Abstractive methods build an internal semantic representation of the original content (often called a language model), and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction. Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge. \"Paraphrasing\" is even more difficult to apply to image and video, which is why most summarization systems are extractive.\n",
    "Aided summarization\n",
    "Approaches aimed at higher summarization quality rely on combined software and human effort. In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bce8faac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy[lookups,transformers] in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (1.21.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (2.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (1.0.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (2.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (21.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (4.64.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (8.1.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (0.7.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (3.0.10)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (65.6.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (1.10.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (1.0.3)\n",
      "Requirement already satisfied: spacy-lookups-data<1.1.0,>=1.0.3 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (1.0.3)\n",
      "Requirement already satisfied: spacy-transformers<1.2.0,>=1.1.2 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy[lookups,transformers]) (1.1.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->spacy[lookups,transformers]) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pathy>=0.3.5->spacy[lookups,transformers]) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy[lookups,transformers]) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy[lookups,transformers]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy[lookups,transformers]) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy[lookups,transformers]) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy[lookups,transformers]) (2022.9.24)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[lookups,transformers]) (0.8.6)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[lookups,transformers]) (1.13.0)\n",
      "Requirement already satisfied: transformers<4.22.0,>=3.4.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[lookups,transformers]) (4.21.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy[lookups,transformers]) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy[lookups,transformers]) (0.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy[lookups,transformers]) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy[lookups,transformers]) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy[lookups,transformers]) (2.1.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[lookups,transformers]) (2022.9.13)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[lookups,transformers]) (0.11.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[lookups,transformers]) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[lookups,transformers]) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[lookups,transformers]) (3.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy[lookups,transformers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fac18441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.21.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ravid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c808d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73769fd",
   "metadata": {},
   "source": [
    "creating stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3defe5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad25e7e",
   "metadata": {},
   "source": [
    "spacy.load is used to load a model. spacy.load('en_core_web_sm') loads the model package en_core_web_sm. This will return a language object nlp containing all components and data needed to process text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "902adfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512ceef",
   "metadata": {},
   "source": [
    "Calling the nlp object on a string of text will return a processed Doc. During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a48a1515",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a75d5",
   "metadata": {},
   "source": [
    "Each Doc consists of individual tokens, and we can iterate over them. Now we will make a list of tokens called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b57eace8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Automatic', 'summarization', 'is', 'the', 'process', 'of', 'shortening', 'a', 'set', 'of', 'data', 'computationally', ',', 'to', 'create', 'a', 'subset', '(', 'a', 'summary', ')', 'that', 'represents', 'the', 'most', 'important', 'or', 'relevant', 'information', 'within', 'the', 'original', 'content', '.', 'Artificial', 'intelligence', 'algorithms', 'are', 'commonly', 'developed', 'and', 'employed', 'to', 'achieve', 'this', ',', 'specialized', 'for', 'different', 'types', 'of', 'data', '.', '\\n\\n', 'Text', 'summarization', 'is', 'usually', 'implemented', 'by', 'natural', 'language', 'processing', 'methods', ',', 'designed', 'to', 'locate', 'the', 'most', 'informative', 'sentences', 'in', 'a', 'given', 'document[1', ']', '.', 'On', 'the', 'other', 'hand', ',', 'visual', 'content', 'can', 'be', 'summarized', 'using', 'computer', 'vision', 'algorithms', '.', 'Image', 'summarization', 'is', 'the', 'subject', 'of', 'ongoing', 'research', ';', 'existing', 'approaches', 'typically', 'attempt', 'to', 'display', 'the', 'most', 'representative', 'images', 'from', 'a', 'given', 'image', 'collection', ',', 'or', 'generate', 'a', 'video', 'that', 'only', 'includes', 'the', 'most', 'important', 'content', 'from', 'the', 'entire', 'collection[2][3][4', ']', '.', 'Video', 'summarization', 'algorithms', 'identify', 'and', 'extract', 'from', 'the', 'original', 'video', 'content', 'the', 'most', 'important', 'frames', '(', 'key', '-', 'frames', ')', ',', 'and/or', 'the', 'most', 'important', 'video', 'segments', '(', 'key', '-', 'shots', ')', ',', 'normally', 'in', 'a', 'temporally', 'ordered', 'fashion[5][6][7][8', ']', '.', 'Video', 'summaries', 'simply', 'retain', 'a', 'carefully', 'selected', 'subset', 'of', 'the', 'original', 'video', 'frames', 'and', ',', 'therefore', ',', 'are', 'not', 'identical', 'to', 'the', 'output', 'of', 'video', 'synopsis', 'algorithms', ',', 'where', 'new', 'video', 'frames', 'are', 'being', 'synthesized', 'based', 'on', 'the', 'original', 'video', 'content', '.', '\\n', 'Extraction', '-', 'based', 'summarization', '\\n', 'Here', ',', 'content', 'is', 'extracted', 'from', 'the', 'original', 'data', ',', 'but', 'the', 'extracted', 'content', 'is', 'not', 'modified', 'in', 'any', 'way', '.', 'Examples', 'of', 'extracted', 'content', 'include', 'key', '-', 'phrases', 'that', 'can', 'be', 'used', 'to', '\"', 'tag', '\"', 'or', 'index', 'a', 'text', 'document', ',', 'or', 'key', 'sentences', '(', 'including', 'headings', ')', 'that', 'collectively', 'comprise', 'an', 'abstract', ',', 'and', 'representative', 'images', 'or', 'video', 'segments', ',', 'as', 'stated', 'above', '.', 'For', 'text', ',', 'extraction', 'is', 'analogous', 'to', 'the', 'process', 'of', 'skimming', ',', 'where', 'the', 'summary', '(', 'if', 'available', ')', ',', 'headings', 'and', 'subheadings', ',', 'figures', ',', 'the', 'first', 'and', 'last', 'paragraphs', 'of', 'a', 'section', ',', 'and', 'optionally', 'the', 'first', 'and', 'last', 'sentences', 'in', 'a', 'paragraph', 'are', 'read', 'before', 'one', 'chooses', 'to', 'read', 'the', 'entire', 'document', 'in', 'detail.[10', ']', 'Other', 'examples', 'of', 'extraction', 'that', 'include', 'key', 'sequences', 'of', 'text', 'in', 'terms', 'of', 'clinical', 'relevance', '(', 'including', 'patient', '/', 'problem', ',', 'intervention', ',', 'and', 'outcome', ')', '.', '\\n', 'Abstractive', '-', 'based', 'summarization', '\\n', 'Abstractive', 'summarization', 'methods', 'generate', 'new', 'text', 'that', 'did', 'not', 'exist', 'in', 'the', 'original', 'text.[12', ']', 'This', 'has', 'been', 'applied', 'mainly', 'for', 'text', '.', 'Abstractive', 'methods', 'build', 'an', 'internal', 'semantic', 'representation', 'of', 'the', 'original', 'content', '(', 'often', 'called', 'a', 'language', 'model', ')', ',', 'and', 'then', 'use', 'this', 'representation', 'to', 'create', 'a', 'summary', 'that', 'is', 'closer', 'to', 'what', 'a', 'human', 'might', 'express', '.', 'Abstraction', 'may', 'transform', 'the', 'extracted', 'content', 'by', 'paraphrasing', 'sections', 'of', 'the', 'source', 'document', ',', 'to', 'condense', 'a', 'text', 'more', 'strongly', 'than', 'extraction', '.', 'Such', 'transformation', ',', 'however', ',', 'is', 'computationally', 'much', 'more', 'challenging', 'than', 'extraction', ',', 'involving', 'both', 'natural', 'language', 'processing', 'and', 'often', 'a', 'deep', 'understanding', 'of', 'the', 'domain', 'of', 'the', 'original', 'text', 'in', 'cases', 'where', 'the', 'original', 'document', 'relates', 'to', 'a', 'special', 'field', 'of', 'knowledge', '.', '\"', 'Paraphrasing', '\"', 'is', 'even', 'more', 'difficult', 'to', 'apply', 'to', 'image', 'and', 'video', ',', 'which', 'is', 'why', 'most', 'summarization', 'systems', 'are', 'extractive', '.', '\\n', 'Aided', 'summarization', '\\n', 'Approaches', 'aimed', 'at', 'higher', 'summarization', 'quality', 'rely', 'on', 'combined', 'software', 'and', 'human', 'effort', '.', 'In', 'Machine', 'Aided', 'Human', 'Summarization', ',', 'extractive', 'techniques', 'highlight', 'candidate', 'passages', 'for', 'inclusion', '(', 'to', 'which', 'the', 'human', 'adds', 'or', 'removes', 'text', ')', '.', 'In', 'Human', 'Aided', 'Machine', 'Summarization', ',', 'a', 'human', 'post', '-', 'processes', 'software', 'output', ',', 'in', 'the', 'same', 'way', 'that', 'one', 'edits', 'the', 'output', 'of', 'automatic', 'translation', 'by', 'Google', 'Translate', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "tokens=[token.text for token in document]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2673bc15",
   "metadata": {},
   "source": [
    "We can see that all the punctuation marks and special characters are included in the tokens. Now we will remove them. punctuation contains a string of all the punctuations but it does now conatin \\n. So we will add \\n in punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3aabdc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = punctuation + '\\n'\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c4a6d",
   "metadata": {},
   "source": [
    "Now we will make the word frequency table. It will contain the number of occurrences of all the distinct words in the text which are not punctuations or stop words. We will create a dictionary named word_frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbdf35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies={}\n",
    "for word in document:\n",
    "    if word.text.lower() not in stopwords:\n",
    "        if word.text.lower() not in punctuation:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text]=1\n",
    "            else:\n",
    "                word_frequencies[word.text]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1db22765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Automatic': 1, 'summarization': 10, 'process': 2, 'shortening': 1, 'set': 1, 'data': 3, 'computationally': 2, 'create': 2, 'subset': 2, 'summary': 3, 'represents': 1, 'important': 4, 'relevant': 1, 'information': 1, 'original': 9, 'content': 10, 'Artificial': 1, 'intelligence': 1, 'algorithms': 4, 'commonly': 1, 'developed': 1, 'employed': 1, 'achieve': 1, 'specialized': 1, 'different': 1, 'types': 1, '\\n\\n': 1, 'Text': 1, 'usually': 1, 'implemented': 1, 'natural': 2, 'language': 3, 'processing': 2, 'methods': 3, 'designed': 1, 'locate': 1, 'informative': 1, 'sentences': 3, 'given': 2, 'document[1': 1, 'hand': 1, 'visual': 1, 'summarized': 1, 'computer': 1, 'vision': 1, 'Image': 1, 'subject': 1, 'ongoing': 1, 'research': 1, 'existing': 1, 'approaches': 1, 'typically': 1, 'attempt': 1, 'display': 1, 'representative': 2, 'images': 2, 'image': 2, 'collection': 1, 'generate': 2, 'video': 9, 'includes': 1, 'entire': 2, 'collection[2][3][4': 1, 'Video': 2, 'identify': 1, 'extract': 1, 'frames': 4, 'key': 5, 'and/or': 1, 'segments': 2, 'shots': 1, 'normally': 1, 'temporally': 1, 'ordered': 1, 'fashion[5][6][7][8': 1, 'summaries': 1, 'simply': 1, 'retain': 1, 'carefully': 1, 'selected': 1, 'identical': 1, 'output': 3, 'synopsis': 1, 'new': 2, 'synthesized': 1, 'based': 3, 'Extraction': 1, 'extracted': 4, 'modified': 1, 'way': 2, 'Examples': 1, 'include': 2, 'phrases': 1, 'tag': 1, 'index': 1, 'text': 8, 'document': 4, 'including': 2, 'headings': 2, 'collectively': 1, 'comprise': 1, 'abstract': 1, 'stated': 1, 'extraction': 4, 'analogous': 1, 'skimming': 1, 'available': 1, 'subheadings': 1, 'figures': 1, 'paragraphs': 1, 'section': 1, 'optionally': 1, 'paragraph': 1, 'read': 2, 'chooses': 1, 'detail.[10': 1, 'examples': 1, 'sequences': 1, 'terms': 1, 'clinical': 1, 'relevance': 1, 'patient': 1, 'problem': 1, 'intervention': 1, 'outcome': 1, 'Abstractive': 3, 'exist': 1, 'text.[12': 1, 'applied': 1, 'mainly': 1, 'build': 1, 'internal': 1, 'semantic': 1, 'representation': 2, 'called': 1, 'model': 1, 'use': 1, 'closer': 1, 'human': 4, 'express': 1, 'Abstraction': 1, 'transform': 1, 'paraphrasing': 1, 'sections': 1, 'source': 1, 'condense': 1, 'strongly': 1, 'transformation': 1, 'challenging': 1, 'involving': 1, 'deep': 1, 'understanding': 1, 'domain': 1, 'cases': 1, 'relates': 1, 'special': 1, 'field': 1, 'knowledge': 1, 'Paraphrasing': 1, 'difficult': 1, 'apply': 1, 'systems': 1, 'extractive': 2, 'Aided': 3, 'Approaches': 1, 'aimed': 1, 'higher': 1, 'quality': 1, 'rely': 1, 'combined': 1, 'software': 2, 'effort': 1, 'Machine': 2, 'Human': 2, 'Summarization': 2, 'techniques': 1, 'highlight': 1, 'candidate': 1, 'passages': 1, 'inclusion': 1, 'adds': 1, 'removes': 1, 'post': 1, 'processes': 1, 'edits': 1, 'automatic': 1, 'translation': 1, 'Google': 1, 'Translate': 1}\n"
     ]
    }
   ],
   "source": [
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07090ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_frequency= max(word_frequencies.values())\n",
    "max_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399eb3a5",
   "metadata": {},
   "source": [
    "We will divide each frequency value in word_frequencies with the max_frequency to normalize the frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de1fd984",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word]=word_frequencies[word]/max_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3df4100f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Automatic': 0.1, 'summarization': 1.0, 'process': 0.2, 'shortening': 0.1, 'set': 0.1, 'data': 0.3, 'computationally': 0.2, 'create': 0.2, 'subset': 0.2, 'summary': 0.3, 'represents': 0.1, 'important': 0.4, 'relevant': 0.1, 'information': 0.1, 'original': 0.9, 'content': 1.0, 'Artificial': 0.1, 'intelligence': 0.1, 'algorithms': 0.4, 'commonly': 0.1, 'developed': 0.1, 'employed': 0.1, 'achieve': 0.1, 'specialized': 0.1, 'different': 0.1, 'types': 0.1, '\\n\\n': 0.1, 'Text': 0.1, 'usually': 0.1, 'implemented': 0.1, 'natural': 0.2, 'language': 0.3, 'processing': 0.2, 'methods': 0.3, 'designed': 0.1, 'locate': 0.1, 'informative': 0.1, 'sentences': 0.3, 'given': 0.2, 'document[1': 0.1, 'hand': 0.1, 'visual': 0.1, 'summarized': 0.1, 'computer': 0.1, 'vision': 0.1, 'Image': 0.1, 'subject': 0.1, 'ongoing': 0.1, 'research': 0.1, 'existing': 0.1, 'approaches': 0.1, 'typically': 0.1, 'attempt': 0.1, 'display': 0.1, 'representative': 0.2, 'images': 0.2, 'image': 0.2, 'collection': 0.1, 'generate': 0.2, 'video': 0.9, 'includes': 0.1, 'entire': 0.2, 'collection[2][3][4': 0.1, 'Video': 0.2, 'identify': 0.1, 'extract': 0.1, 'frames': 0.4, 'key': 0.5, 'and/or': 0.1, 'segments': 0.2, 'shots': 0.1, 'normally': 0.1, 'temporally': 0.1, 'ordered': 0.1, 'fashion[5][6][7][8': 0.1, 'summaries': 0.1, 'simply': 0.1, 'retain': 0.1, 'carefully': 0.1, 'selected': 0.1, 'identical': 0.1, 'output': 0.3, 'synopsis': 0.1, 'new': 0.2, 'synthesized': 0.1, 'based': 0.3, 'Extraction': 0.1, 'extracted': 0.4, 'modified': 0.1, 'way': 0.2, 'Examples': 0.1, 'include': 0.2, 'phrases': 0.1, 'tag': 0.1, 'index': 0.1, 'text': 0.8, 'document': 0.4, 'including': 0.2, 'headings': 0.2, 'collectively': 0.1, 'comprise': 0.1, 'abstract': 0.1, 'stated': 0.1, 'extraction': 0.4, 'analogous': 0.1, 'skimming': 0.1, 'available': 0.1, 'subheadings': 0.1, 'figures': 0.1, 'paragraphs': 0.1, 'section': 0.1, 'optionally': 0.1, 'paragraph': 0.1, 'read': 0.2, 'chooses': 0.1, 'detail.[10': 0.1, 'examples': 0.1, 'sequences': 0.1, 'terms': 0.1, 'clinical': 0.1, 'relevance': 0.1, 'patient': 0.1, 'problem': 0.1, 'intervention': 0.1, 'outcome': 0.1, 'Abstractive': 0.3, 'exist': 0.1, 'text.[12': 0.1, 'applied': 0.1, 'mainly': 0.1, 'build': 0.1, 'internal': 0.1, 'semantic': 0.1, 'representation': 0.2, 'called': 0.1, 'model': 0.1, 'use': 0.1, 'closer': 0.1, 'human': 0.4, 'express': 0.1, 'Abstraction': 0.1, 'transform': 0.1, 'paraphrasing': 0.1, 'sections': 0.1, 'source': 0.1, 'condense': 0.1, 'strongly': 0.1, 'transformation': 0.1, 'challenging': 0.1, 'involving': 0.1, 'deep': 0.1, 'understanding': 0.1, 'domain': 0.1, 'cases': 0.1, 'relates': 0.1, 'special': 0.1, 'field': 0.1, 'knowledge': 0.1, 'Paraphrasing': 0.1, 'difficult': 0.1, 'apply': 0.1, 'systems': 0.1, 'extractive': 0.2, 'Aided': 0.3, 'Approaches': 0.1, 'aimed': 0.1, 'higher': 0.1, 'quality': 0.1, 'rely': 0.1, 'combined': 0.1, 'software': 0.2, 'effort': 0.1, 'Machine': 0.2, 'Human': 0.2, 'Summarization': 0.2, 'techniques': 0.1, 'highlight': 0.1, 'candidate': 0.1, 'passages': 0.1, 'inclusion': 0.1, 'adds': 0.1, 'removes': 0.1, 'post': 0.1, 'processes': 0.1, 'edits': 0.1, 'automatic': 0.1, 'translation': 0.1, 'Google': 0.1, 'Translate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58274dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content., Artificial intelligence algorithms are commonly developed and employed to achieve this, specialized for different types of data.\n",
      "\n",
      ", Text summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document[1]., On the other hand, visual content can be summarized using computer vision algorithms., Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection[2][3][4]., Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion[5][6][7][8]., Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.\n",
      ", Extraction-based summarization\n",
      "Here, content is extracted from the original data, but the extracted content is not modified in any way., Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above., For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).\n",
      ", Abstractive-based summarization\n",
      "Abstractive summarization methods generate new text that did not exist in the original text.[12], This has been applied mainly for text., Abstractive methods build an internal semantic representation of the original content (often called a language model), and then use this representation to create a summary that is closer to what a human might express., Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction., Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge., \"Paraphrasing\" is even more difficult to apply to image and video, which is why most summarization systems are extractive.\n",
      ", Aided summarization\n",
      "Approaches aimed at higher summarization quality rely on combined software and human effort., In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text)., In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens=[sent for sent in document.sents]\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24766e",
   "metadata": {},
   "source": [
    "Now we will calculate the sentence scores. The sentence score for a particular sentence is the sum of the normalized frequencies of the words in that sentence. All the sentences will be stored with their score in the dictionary sentence_scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e726301",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_score={}\n",
    "for sent in sentence_tokens:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_frequencies.keys():\n",
    "            if sent not in sentence_score.keys():\n",
    "                sentence_score[sent]=word_frequencies[word.text.lower()]\n",
    "            else:\n",
    "                sentence_score[sent]+=word_frequencies[word.text.lower()]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "749eda0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.: 5.300000000000001,\n",
       " Artificial intelligence algorithms are commonly developed and employed to achieve this, specialized for different types of data.\n",
       " : 1.6,\n",
       " Text summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document[1].: 3.9000000000000004,\n",
       " On the other hand, visual content can be summarized using computer vision algorithms.: 1.9000000000000004,\n",
       " Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection[2][3][4].: 5.800000000000002,\n",
       " Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion[5][6][7][8].: 9.599999999999998,\n",
       " Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.: 10.3,\n",
       " Extraction-based summarization\n",
       " Here, content is extracted from the original data, but the extracted content is not modified in any way.: 6.0,\n",
       " Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above.: 6.800000000000001,\n",
       " For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).: 7.299999999999999,\n",
       " Abstractive-based summarization\n",
       " Abstractive summarization methods generate new text that did not exist in the original text.[12]: 4.8999999999999995,\n",
       " This has been applied mainly for text.: 1.0,\n",
       " Abstractive methods build an internal semantic representation of the original content (often called a language model), and then use this representation to create a summary that is closer to what a human might express.: 4.6000000000000005,\n",
       " Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction.: 3.6000000000000005,\n",
       " Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.: 5.3999999999999995,\n",
       " \"Paraphrasing\" is even more difficult to apply to image and video, which is why most summarization systems are extractive.: 2.7,\n",
       " Aided summarization\n",
       " Approaches aimed at higher summarization quality rely on combined software and human effort.: 3.3000000000000007,\n",
       " In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text).: 3.5,\n",
       " In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.: 3.3000000000000003}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6faf28",
   "metadata": {},
   "source": [
    "Now we are going to select 30% of the sentences having the largest scores. For this we are going to import nlargest from heapq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82b377f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9651f64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_length =int(len(sentence_tokens)*0.3)\n",
    "select_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c631e3bf",
   "metadata": {},
   "source": [
    "nlargest() will return a list with the select_length largest elements i.e. 4 largest elements from sentence_scores. key = sentence_scores.get specifies a function of one argument that is used to extract a comparison key from each element in sentence_scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bdee5b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.,\n",
       " Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion[5][6][7][8].,\n",
       " For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).,\n",
       " Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above.,\n",
       " Extraction-based summarization\n",
       " Here, content is extracted from the original data, but the extracted content is not modified in any way.]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary=nlargest(select_length,sentence_score,key=sentence_score.get)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae37b1b",
   "metadata": {},
   "source": [
    "Now we will combine this sentence together and make final string which contains the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe341e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.\\n',\n",
       " 'Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion[5][6][7][8].',\n",
       " 'For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).\\n',\n",
       " 'Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above.',\n",
       " 'Extraction-based summarization\\nHere, content is extracted from the original data, but the extracted content is not modified in any way.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_summary=[word.text for word in summary]\n",
    "final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e78732a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.\\n Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion[5][6][7][8]. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).\\n Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. Extraction-based summarization\\nHere, content is extracted from the original data, but the extracted content is not modified in any way.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary= ' '.join(final_summary)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65e7e1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3514"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf9eebf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1314"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c5d4e0",
   "metadata": {},
   "source": [
    "\"credits-KGP Talkie\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82febcd5",
   "metadata": {},
   "source": [
    "NLP Application: Named Entity Recognition (NER) in Python with Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0dd12c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text=\"The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well. informally known as Mangalyaan.Earth orbit on 5 November 2013 by the Indian Space Research Organisation (ISRO),It was completed at a record low cost of $74 million.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9fba54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1= NER(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7c22492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Indian Space Research Organisation ORG\n",
      "India GPE\n",
      "Bengaluru GPE\n",
      "under Department of Space ORG\n",
      "India GPE\n",
      "Mangalyaan PERSON\n",
      "Earth LOC\n",
      "5 November 2013 DATE\n",
      "the Indian Space Research Organisation ORG\n",
      "$74 million MONEY\n"
     ]
    }
   ],
   "source": [
    "for word in text1.ents:\n",
    "    print(word.text,word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dolo', 'ORG'), ('expired', 'Condition')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "states = ['not expired', 'expired']\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')  # or any other model\n",
    "patterns = [nlp(state) for state in states]  # process each word to create phrase pattern\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add('Condition', None, *patterns)  # add patterns to matcher\n",
    "doc = nlp(\"Dolo is expired\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    # create a new Span for each match and use the match_id (Condition) as the label\n",
    "    span = Span(doc, start, end, label=match_id)\n",
    "    doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])  # [('expired', 'Comndition')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6105d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "248d5cf0309dbd7e9180d8b0456a70a7480971f1282479808058ab10f9040c64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
